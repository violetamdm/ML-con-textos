{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frases empalagosas San Valentin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports y downloads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports (modulos usados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow\n",
    "import nltk #Para el preprocesamiento\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.arlstem import ARLSTem\n",
    "from nltk.stem.arlstem2 import ARLSTem2\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy \n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token, DocBin\n",
    "# from spacy.lang.es.examples import sentences # Frases en español\n",
    "from spacy.lang.es import Spanish\n",
    "# Ejemplo de instancia que funciona del elenguaje español usandoi spacy:\n",
    "# x = Spanish()\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "import tflearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import stanza\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargas:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/mcd-unison/primeros-pasos-con-clasificaci%C3%B3n-de-textos-en-espa%C3%B1ol-con-spacy-c36b46acf65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación, preprocesamiento y análisis de datos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de los datos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizar: Crear una lista de palablas (Tokens) a partir de un texto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Español:\n",
    "- str_e    ->  String español \n",
    "- lista_e  ->  Lista tokens español\n",
    "\n",
    "Ingles:\n",
    "- str_i    ->  String español \n",
    "- lista_e  ->  Lista tokens español"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto español"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_e = [[\"¡Te deseo un día de San Valentín increíble, mi amor, sueña con un futuro a mi lado! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad que se quede en nuestros corazones\"], \n",
    "[\"¡Feliz día de San Valentín, mi vida! quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón. Te amo con locura\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_e2 = [\"Ten un día de San Valentín estupendo, sigamosconstruyendo nuestro amor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_e = [\"Muchas gracias, yo a tí tambien\", \"¿que has dicho?\", \"felicidades a tí también\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡Te deseo un día de San Valentín increíble, mi amor, sueña con un futuro a mi lado! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad que se quede en nuestros corazones', ' ', '¡Feliz día de San Valentín, mi vida! quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón. Te amo con locura']\n"
     ]
    }
   ],
   "source": [
    "t_e = []\n",
    "\n",
    "for i in range(0, len(texto_e)):\n",
    "    if(i>0):\n",
    "        t_e =t_e + [\" \"]\n",
    "    t_e = t_e + texto_e[i]\n",
    "\n",
    "print(t_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Te deseo un día de San Valentín increíble, mi amor, sueña con un futuro a mi lado! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad que se quede en nuestros corazones ¡Feliz día de San Valentín, mi vida! quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón. Te amo con locura\n"
     ]
    }
   ],
   "source": [
    "str_e = \"\"\n",
    "\n",
    "for i in range(0, len(t_e)):\n",
    "    str_e = str_e + str(t_e[i])\n",
    "print(str_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://micro.recursospython.com/recursos/como-quitar-tildes-de-una-cadena.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quitar signos de puntuacion y dejar las letras en minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te deseo un día de san valentín increíble mi amor sueña con un futuro a mi lado sigamos avanzando y trabajando por hacer de nuestros sueños una realidad que se quede en nuestros corazones feliz día de san valentín mi vida quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón te amo con locura\n"
     ]
    }
   ],
   "source": [
    "# También se puede usar esto\n",
    "import string\n",
    "string.punctuation\n",
    "str_aux = \"\"\n",
    " # end used to print the output in one line\n",
    "for i in str_e:\n",
    "    if i in (string.punctuation+\"¡¿\"):\n",
    "        i = \"\" \n",
    "    str_aux = str_aux + i.lower()           # checking for occurence of any punctuation\n",
    "str_e = str_aux\n",
    "print(str_e)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "spanish_stopwords = stopwords.words('spanish')\n",
    "print(spanish_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deseo día san valentín increíble amor sueña futuro lado sigamos avanzando trabajando hacer sueños realidad quede corazones feliz día san valentín vida quiero sepas cuanto valoro gran lugar ocupas corazón amo locura\n"
     ]
    }
   ],
   "source": [
    "str_e = \" \" + str_e + \" \"\n",
    "for i in range(0, len(spanish_stopwords)):\n",
    "    if \" \" + spanish_stopwords[i] + \" \" in str_e:\n",
    "        str_e = str_e.replace(\" \"+ spanish_stopwords[i] + \" \", \" \")\n",
    "str_e = str_e.strip()\n",
    "print(str_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quitar tildes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deseo dia san valentin increible amor sueña futuro lado sigamos avanzando trabajando hacer sueños realidad quede corazones feliz dia san valentin vida quiero sepas cuanto valoro gran lugar ocupas corazon amo locura\n"
     ]
    }
   ],
   "source": [
    "tildes = (\n",
    "    (\"á\", \"a\"),\n",
    "    (\"é\", \"e\"),\n",
    "    (\"í\", \"i\"),\n",
    "    (\"ó\", \"o\"),\n",
    "    (\"ú\", \"u\"),\n",
    ")\n",
    "for letra_tilde, letra in tildes:\n",
    "    str_e = str_e.replace(letra_tilde, letra).replace(letra.upper(), letra_tilde.upper())\n",
    "\n",
    "print(str_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deseo', 'dia', 'san', 'valentin', 'increible', 'amor', 'sueña', 'futuro', 'lado', 'sigamos', 'avanzando', 'trabajando', 'hacer', 'sueños', 'realidad', 'quede', 'corazones', 'feliz', 'dia', 'san', 'valentin', 'vida', 'quiero', 'sepas', 'cuanto', 'valoro', 'gran', 'lugar', 'ocupas', 'corazon', 'amo', 'locura']\n"
     ]
    }
   ],
   "source": [
    "lista_e = str_e.split()\n",
    "print(lista_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NO ME SALIÓ : Tokenizar + quitar signos de puntuación (no me lo ví bien deja espacios en blanco porque sí)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import re\\n# cargando el texto\\nlista_e = re.split(r'\\\\W+', str_e.lower())\\nprint(lista_e)\""
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import re\n",
    "# cargando el texto\n",
    "lista_e = re.split(r'\\W+', str_e.lower())\n",
    "print(lista_e)\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto ingles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear texto en ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_i = [[\"I love you.\"], [\"A hundred hearts would be too few to carry all my love for you.\"], [\"If you were thinking about someone while reading this, you're definitely in love.\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love you.', ' ', 'A hundred hearts would be too few to carry all my love for you.', ' ', \"If you were thinking about someone while reading this, you're definitely in love.\"]\n"
     ]
    }
   ],
   "source": [
    "t_i = []\n",
    "for i in range(0, len(texto_i)):\n",
    "    if(i>0):\n",
    "        t_i =t_i + [\" \"]\n",
    "    t_i = t_i + texto_i[i]\n",
    "\n",
    "print(t_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = stopwords.words('english')\n",
    "print(english_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear string en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love you. A hundred hearts would be too few to carry all my love for you. If you were thinking about someone while reading this, you're definitely in love.\n"
     ]
    }
   ],
   "source": [
    "str_i = \"\"\n",
    "for i in range(0, len(t_i)):\n",
    "    str_i = str_i + str(t_i[i])\n",
    "print(str_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quitar signos de puntuacion y dejar las letras en minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you a hundred hearts would be too few to carry all my love for you if you were thinking about someone while reading this youre definitely in love\n"
     ]
    }
   ],
   "source": [
    "# También se puede usar esto\n",
    "import string\n",
    "string.punctuation\n",
    "str_aux = \"\"\n",
    " # end used to print the output in one line\n",
    "for i in str_i:\n",
    "    if i in (string.punctuation+\"¡\"):\n",
    "        i = \"\" \n",
    "    str_aux = str_aux + i.lower()           # checking for occurence of any punctuation\n",
    "str_i = str_aux\n",
    "print(str_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'you', 'a', 'hundred', 'hearts', 'would', 'be', 'too', 'few', 'to', 'carry', 'all', 'my', 'love', 'for', 'you', 'if', 'you', 'were', 'thinking', 'about', 'someone', 'while', 'reading', 'this', 'youre', 'definitely', 'in', 'love']\n"
     ]
    }
   ],
   "source": [
    "lista_i = str_i.split()\n",
    "print(lista_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NO ME SALIÓ : Tokenizar + quitar signos de puntuación (no me lo ví bien deja espacios en blanco porque sí)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import re\\n# cargando el texto\\nlista_i = re.split(r'\\\\W+', str_i.lower())\\nprint(lista_i)\""
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import re\n",
    "# cargando el texto\n",
    "lista_i = re.split(r'\\W+', str_i.lower())\n",
    "print(lista_i)\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniza un texto a través de la creación de un vocabulario con las palabras que éste contiene.\n",
    "<br><br>\n",
    "Realiza una transformación cuyo objetivo es obtener un mejor tratamiento de los datos.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texto = [[\"¡Te deseo un día de San Valentín increíble, mi amor! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad\"], [\"\"]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de palabras "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista con el texto a normalizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡Te deseo un día de San Valentín increíble, mi amor, sueña con un futuro a mi lado! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad que se quede en nuestros corazones', ' ', '¡Feliz día de San Valentín, mi vida! quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón. Te amo con locura']\n"
     ]
    }
   ],
   "source": [
    "print(t_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el count_vectorizer con las stopwords españolas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=spanish_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_spanish = count_vectorizer.fit_transform(t_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(text_spanish.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amo', 'amor', 'avanzando', 'corazones', 'corazón', 'cuanto',\n",
       "       'deseo', 'día', 'feliz', 'futuro', 'gran', 'hacer', 'increíble',\n",
       "       'lado', 'locura', 'lugar', 'ocupas', 'quede', 'quiero', 'realidad',\n",
       "       'san', 'sepas', 'sigamos', 'sueña', 'sueños', 'trabajando',\n",
       "       'valentín', 'valoro', 'vida'], dtype=object)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx1 = count_vectorizer.fit(t_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.educative.io/answers/countvectorizer-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deseo': 6, 'día': 7, 'san': 20, 'valentín': 26, 'increíble': 12, 'amor': 1, 'sueña': 23, 'futuro': 9, 'lado': 13, 'sigamos': 22, 'avanzando': 2, 'trabajando': 25, 'hacer': 11, 'sueños': 24, 'realidad': 19, 'quede': 17, 'corazones': 3, 'feliz': 8, 'vida': 28, 'quiero': 18, 'sepas': 21, 'cuanto': 5, 'valoro': 27, 'gran': 10, 'lugar': 15, 'ocupas': 16, 'corazón': 4, 'amo': 0, 'locura': 14}\n"
     ]
    }
   ],
   "source": [
    "print(tx1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['k', 'j', 'h', 'f', 'g', 'd', 'j', 'k', 'g', 'k', 'd', 'j', 'f', 'g', 'h', 'g', 'j', 'k', 'b', 'd', 'f', 'k', 'j', 'h'], []]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kk'], []]\n"
     ]
    }
   ],
   "source": [
    "l = [[],[]]\n",
    "l[0].append(\"kk\")\n",
    "print(l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es para saber si una palabra del array mostrado en la anterior está presente en esa frase:\n",
    "- 0 = no está\n",
    "- 1 = está"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(text_spanish.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " «one-hot encoding»Apariciones de cada token, en una tabla que nos permita saber qué palabras hay y cuántas veces aparecieron en el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 29)\n"
     ]
    }
   ],
   "source": [
    "print(text_spanish.shape) # Muestra las dimensiones de la variable vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf vectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strip_accents ---> Sirve para eliminar los acentos (tildes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cienciadedatos.net/documentos/py25-text-mining-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=spanish_stopwords, strip_accents=\"ascii\")\n",
    "X = vectorizer.fit_transform(t_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function _analyze at 0x00000218EA40D760>, ngrams=<bound method _VectorizerMixin._word_ngrams of TfidfVectorizer(stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
       "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
       "                            'no', 'una', 'su', 'al', 'lo', 'como', 'más',\n",
       "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí',\n",
       "                            'porque', ...],\n",
       "                strip_accents='ascii')>, tokenizer=<built-in method findall of re.Pattern object at 0x00000218F5C903C0>, preprocessor=functools.partial(<function _preprocess at 0x00000218EA40D3A0>, accent_function=<function strip_accents_ascii at 0x00000218EA40DB20>, lower=True), decoder=<bound method _VectorizerMixin.decode of TfidfVectorizer(stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
       "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
       "                            'no', 'una', 'su', 'al', 'lo', 'como', 'más',\n",
       "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí',\n",
       "                            'porque', ...],\n",
       "                strip_accents='ascii')>, stop_words=frozenset({'seamos', 'sería', 'hubieran', 'tuviese', 'mucho', 'vuestra', 'estarán', 'habrán', 'me', 'tenida', 'tuvimos', 'estos', 'seas', 'se', 'habido', 'tuvieran', 'en', 'otros', 'sean', 'todo', 'con', 'cuando', 'ni', 'tendrás', 'serían', 'él', 'haya', 'nosotros', 'nos', 'entre', 'a', 'una', 'mía', 'y', 'fuese', 'seréis', 'estemos', 'fuera', 'hasta', 'ellas', 'estaba', 'esos', 'habida', 'estaremos', 'tus', 'hayamos', 'unos', 'estuviésemos', 'habríamos', 'tuve', 'la', 'vuestros', 'estaréis', 'sus', 'estados', 'que', 'sentid', 'hubieras', 'muchos', 'vuestro', 'cual', 'fuesen', 'habéis', 'pero', 'estuviera', 'habidas', 'tuvieseis', 'fueses', 'estarían', 'sois', 'esas', 'tuviesen', 'estado', 'lo', 'tú', 'fuésemos', 'habías', 'ti', 'desde', 'habrían', 'vuestras', 'seré', 'tendrán', 'también', 'habiendo', 'nuestros', 'ellos', 'nuestras', 'míos', 'hube', 'habrías', 'hay', 'tenemos', 'nuestro', 'será', 'sentidos', 'tuviera', 'suyas', 'tengan', 'no', 'hubiste', 'tuyos', 'estarías', 'porque', 'otro', 'ante', 'tengas', 'estuviese', 'estáis', 'estén', 'muy', 'su', 'estuvieras', 'fuisteis', 'suya', 'estás', 'estés', 'fuimos', 'donde', 'esta', 'tiene', 'estuvieron', 'algunas', 'sentida', 'sobre', 'antes', 'estuvieran', 'hubiéramos', 'teníamos', 'todos', 'tenéis', 'tenía', 'son', 'hubieses', 'hubiese', 'suyo', 'tuya', 'tu', 'sintiendo', 'estaríais', 'teníais', 'hayáis', 'habíais', 'fuerais', 'tendríais', 'fui', 'tendrían', 'nosotras', 'habían', 'habréis', 'como', 'estabais', 'habremos', 'fuéramos', 'tuyas', 'fueseis', 'un', 'tanto', 'habíamos', 'tendríamos', 'contra', 'habrás', 'estando', 'fue', 'hubiésemos', 'tuviésemos', 'estuvo', 'mío', 'mías', 'eres', 'tendréis', 'esté', 'estábamos', 'somos', 'es', 'tuviste', 'tuyo', 'hemos', 'algo', 'tuvisteis', 'fueron', 'para', 'los', 'te', 'tendrías', 'qué', 'tenían', 'ese', 'están', 'habrá', 'tendrá', 'durante', 'tengo', 'mi', 'mí', 'habré', 'tengáis', 'estabas', 'tienen', 'seáis', 'tuviéramos', 'hayas', 'fuiste', 'hubiesen', 'seríamos', 'he', 'este', 'quienes', 'estuvieseis', 'eran', 'tenga', 'has', 'sin', 'algunos', 'el', 'le', 'hubimos', 'tened', 'tienes', 'era', 'por', 'del', 'tenidos', 'más', 'o', 'otra', 'sí', 'las', 'fueras', 'eras', 'estoy', 'fueran', 'sentidas', 'esto', 'estar', 'tuvieses', 'nuestra', 'estad', 'otras', 'habría', 'siente', 'estuve', 'estada', 'éramos', 'yo', 'hayan', 'tengamos', 'estadas', 'había', 'estaríamos', 'tenidas', 'estamos', 'estuviéramos', 'tendría', 'han', 'hubo', 'eso', 'vosotras', 'estéis', 'estuviste', 'serán', 'estas', 'tendremos', 'uno', 'al', 'hubieron', 'esa', 'ella', 'soy', 'tuvieron', 'quien', 'estuvierais', 'ha', 'tuvierais', 'sea', 'habríais', 'habidos', 'tendré', 'seremos', 'hubiera', 'suyos', 'serías', 'e', 'estaban', 'de', 'poco', 'estaría', 'hubierais', 'ya', 'hubisteis', 'tuvo', 'estuvimos', 'os', 'estarás', 'estuviesen', 'seríais', 'está', 'teniendo', 'nada', 'erais', 'tenías', 'sentido', 'serás', 'tuvieras', 'estuvieses', 'vosotros', 'les', 'estaré', 'estará', 'mis', 'estuvisteis', 'hubieseis', 'tenido'}))"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.build_analyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function _preprocess at 0x00000218EA40D3A0>, accent_function=<function strip_accents_ascii at 0x00000218EA40DB20>, lower=True)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.build_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Pattern.findall(string, pos=0, endpos=9223372036854775807)>"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amo', 'amor', 'avanzando', 'corazon', 'corazones', 'cuanto',\n",
       "       'deseo', 'dia', 'feliz', 'futuro', 'gran', 'hacer', 'increible',\n",
       "       'lado', 'locura', 'lugar', 'ocupas', 'quede', 'quiero', 'realidad',\n",
       "       'san', 'sepas', 'sigamos', 'suena', 'suenos', 'trabajando',\n",
       "       'valentin', 'valoro', 'vida'], dtype=object)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.float64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'norm': 'l2',\n",
       " 'preprocessor': None,\n",
       " 'smooth_idf': True,\n",
       " 'stop_words': ['de',\n",
       "  'la',\n",
       "  'que',\n",
       "  'el',\n",
       "  'en',\n",
       "  'y',\n",
       "  'a',\n",
       "  'los',\n",
       "  'del',\n",
       "  'se',\n",
       "  'las',\n",
       "  'por',\n",
       "  'un',\n",
       "  'para',\n",
       "  'con',\n",
       "  'no',\n",
       "  'una',\n",
       "  'su',\n",
       "  'al',\n",
       "  'lo',\n",
       "  'como',\n",
       "  'más',\n",
       "  'pero',\n",
       "  'sus',\n",
       "  'le',\n",
       "  'ya',\n",
       "  'o',\n",
       "  'este',\n",
       "  'sí',\n",
       "  'porque',\n",
       "  'esta',\n",
       "  'entre',\n",
       "  'cuando',\n",
       "  'muy',\n",
       "  'sin',\n",
       "  'sobre',\n",
       "  'también',\n",
       "  'me',\n",
       "  'hasta',\n",
       "  'hay',\n",
       "  'donde',\n",
       "  'quien',\n",
       "  'desde',\n",
       "  'todo',\n",
       "  'nos',\n",
       "  'durante',\n",
       "  'todos',\n",
       "  'uno',\n",
       "  'les',\n",
       "  'ni',\n",
       "  'contra',\n",
       "  'otros',\n",
       "  'ese',\n",
       "  'eso',\n",
       "  'ante',\n",
       "  'ellos',\n",
       "  'e',\n",
       "  'esto',\n",
       "  'mí',\n",
       "  'antes',\n",
       "  'algunos',\n",
       "  'qué',\n",
       "  'unos',\n",
       "  'yo',\n",
       "  'otro',\n",
       "  'otras',\n",
       "  'otra',\n",
       "  'él',\n",
       "  'tanto',\n",
       "  'esa',\n",
       "  'estos',\n",
       "  'mucho',\n",
       "  'quienes',\n",
       "  'nada',\n",
       "  'muchos',\n",
       "  'cual',\n",
       "  'poco',\n",
       "  'ella',\n",
       "  'estar',\n",
       "  'estas',\n",
       "  'algunas',\n",
       "  'algo',\n",
       "  'nosotros',\n",
       "  'mi',\n",
       "  'mis',\n",
       "  'tú',\n",
       "  'te',\n",
       "  'ti',\n",
       "  'tu',\n",
       "  'tus',\n",
       "  'ellas',\n",
       "  'nosotras',\n",
       "  'vosotros',\n",
       "  'vosotras',\n",
       "  'os',\n",
       "  'mío',\n",
       "  'mía',\n",
       "  'míos',\n",
       "  'mías',\n",
       "  'tuyo',\n",
       "  'tuya',\n",
       "  'tuyos',\n",
       "  'tuyas',\n",
       "  'suyo',\n",
       "  'suya',\n",
       "  'suyos',\n",
       "  'suyas',\n",
       "  'nuestro',\n",
       "  'nuestra',\n",
       "  'nuestros',\n",
       "  'nuestras',\n",
       "  'vuestro',\n",
       "  'vuestra',\n",
       "  'vuestros',\n",
       "  'vuestras',\n",
       "  'esos',\n",
       "  'esas',\n",
       "  'estoy',\n",
       "  'estás',\n",
       "  'está',\n",
       "  'estamos',\n",
       "  'estáis',\n",
       "  'están',\n",
       "  'esté',\n",
       "  'estés',\n",
       "  'estemos',\n",
       "  'estéis',\n",
       "  'estén',\n",
       "  'estaré',\n",
       "  'estarás',\n",
       "  'estará',\n",
       "  'estaremos',\n",
       "  'estaréis',\n",
       "  'estarán',\n",
       "  'estaría',\n",
       "  'estarías',\n",
       "  'estaríamos',\n",
       "  'estaríais',\n",
       "  'estarían',\n",
       "  'estaba',\n",
       "  'estabas',\n",
       "  'estábamos',\n",
       "  'estabais',\n",
       "  'estaban',\n",
       "  'estuve',\n",
       "  'estuviste',\n",
       "  'estuvo',\n",
       "  'estuvimos',\n",
       "  'estuvisteis',\n",
       "  'estuvieron',\n",
       "  'estuviera',\n",
       "  'estuvieras',\n",
       "  'estuviéramos',\n",
       "  'estuvierais',\n",
       "  'estuvieran',\n",
       "  'estuviese',\n",
       "  'estuvieses',\n",
       "  'estuviésemos',\n",
       "  'estuvieseis',\n",
       "  'estuviesen',\n",
       "  'estando',\n",
       "  'estado',\n",
       "  'estada',\n",
       "  'estados',\n",
       "  'estadas',\n",
       "  'estad',\n",
       "  'he',\n",
       "  'has',\n",
       "  'ha',\n",
       "  'hemos',\n",
       "  'habéis',\n",
       "  'han',\n",
       "  'haya',\n",
       "  'hayas',\n",
       "  'hayamos',\n",
       "  'hayáis',\n",
       "  'hayan',\n",
       "  'habré',\n",
       "  'habrás',\n",
       "  'habrá',\n",
       "  'habremos',\n",
       "  'habréis',\n",
       "  'habrán',\n",
       "  'habría',\n",
       "  'habrías',\n",
       "  'habríamos',\n",
       "  'habríais',\n",
       "  'habrían',\n",
       "  'había',\n",
       "  'habías',\n",
       "  'habíamos',\n",
       "  'habíais',\n",
       "  'habían',\n",
       "  'hube',\n",
       "  'hubiste',\n",
       "  'hubo',\n",
       "  'hubimos',\n",
       "  'hubisteis',\n",
       "  'hubieron',\n",
       "  'hubiera',\n",
       "  'hubieras',\n",
       "  'hubiéramos',\n",
       "  'hubierais',\n",
       "  'hubieran',\n",
       "  'hubiese',\n",
       "  'hubieses',\n",
       "  'hubiésemos',\n",
       "  'hubieseis',\n",
       "  'hubiesen',\n",
       "  'habiendo',\n",
       "  'habido',\n",
       "  'habida',\n",
       "  'habidos',\n",
       "  'habidas',\n",
       "  'soy',\n",
       "  'eres',\n",
       "  'es',\n",
       "  'somos',\n",
       "  'sois',\n",
       "  'son',\n",
       "  'sea',\n",
       "  'seas',\n",
       "  'seamos',\n",
       "  'seáis',\n",
       "  'sean',\n",
       "  'seré',\n",
       "  'serás',\n",
       "  'será',\n",
       "  'seremos',\n",
       "  'seréis',\n",
       "  'serán',\n",
       "  'sería',\n",
       "  'serías',\n",
       "  'seríamos',\n",
       "  'seríais',\n",
       "  'serían',\n",
       "  'era',\n",
       "  'eras',\n",
       "  'éramos',\n",
       "  'erais',\n",
       "  'eran',\n",
       "  'fui',\n",
       "  'fuiste',\n",
       "  'fue',\n",
       "  'fuimos',\n",
       "  'fuisteis',\n",
       "  'fueron',\n",
       "  'fuera',\n",
       "  'fueras',\n",
       "  'fuéramos',\n",
       "  'fuerais',\n",
       "  'fueran',\n",
       "  'fuese',\n",
       "  'fueses',\n",
       "  'fuésemos',\n",
       "  'fueseis',\n",
       "  'fuesen',\n",
       "  'sintiendo',\n",
       "  'sentido',\n",
       "  'sentida',\n",
       "  'sentidos',\n",
       "  'sentidas',\n",
       "  'siente',\n",
       "  'sentid',\n",
       "  'tengo',\n",
       "  'tienes',\n",
       "  'tiene',\n",
       "  'tenemos',\n",
       "  'tenéis',\n",
       "  'tienen',\n",
       "  'tenga',\n",
       "  'tengas',\n",
       "  'tengamos',\n",
       "  'tengáis',\n",
       "  'tengan',\n",
       "  'tendré',\n",
       "  'tendrás',\n",
       "  'tendrá',\n",
       "  'tendremos',\n",
       "  'tendréis',\n",
       "  'tendrán',\n",
       "  'tendría',\n",
       "  'tendrías',\n",
       "  'tendríamos',\n",
       "  'tendríais',\n",
       "  'tendrían',\n",
       "  'tenía',\n",
       "  'tenías',\n",
       "  'teníamos',\n",
       "  'teníais',\n",
       "  'tenían',\n",
       "  'tuve',\n",
       "  'tuviste',\n",
       "  'tuvo',\n",
       "  'tuvimos',\n",
       "  'tuvisteis',\n",
       "  'tuvieron',\n",
       "  'tuviera',\n",
       "  'tuvieras',\n",
       "  'tuviéramos',\n",
       "  'tuvierais',\n",
       "  'tuvieran',\n",
       "  'tuviese',\n",
       "  'tuvieses',\n",
       "  'tuviésemos',\n",
       "  'tuvieseis',\n",
       "  'tuviesen',\n",
       "  'teniendo',\n",
       "  'tenido',\n",
       "  'tenida',\n",
       "  'tenidos',\n",
       "  'tenidas',\n",
       "  'tened'],\n",
       " 'strip_accents': 'ascii',\n",
       " 'sublinear_tf': False,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'use_idf': True,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x29 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 32 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(t_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 29)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 26)\t0.19172459387142654\n",
      "  (0, 25)\t0.2520948008069455\n",
      "  (0, 24)\t0.2520948008069455\n",
      "  (0, 23)\t0.2520948008069455\n",
      "  (0, 22)\t0.2520948008069455\n",
      "  (0, 20)\t0.19172459387142654\n",
      "  (0, 19)\t0.2520948008069455\n",
      "  (0, 17)\t0.2520948008069455\n",
      "  (0, 13)\t0.2520948008069455\n",
      "  (0, 12)\t0.2520948008069455\n",
      "  (0, 11)\t0.2520948008069455\n",
      "  (0, 9)\t0.2520948008069455\n",
      "  (0, 7)\t0.19172459387142654\n",
      "  (0, 6)\t0.2520948008069455\n",
      "  (0, 3)\t0.2520948008069455\n",
      "  (0, 2)\t0.2520948008069455\n",
      "  (0, 1)\t0.2520948008069455\n",
      "  (2, 28)\t0.26982521515445296\n",
      "  (2, 27)\t0.26982521515445296\n",
      "  (2, 26)\t0.2052090309921715\n",
      "  (2, 21)\t0.26982521515445296\n",
      "  (2, 20)\t0.2052090309921715\n",
      "  (2, 18)\t0.26982521515445296\n",
      "  (2, 16)\t0.26982521515445296\n",
      "  (2, 15)\t0.26982521515445296\n",
      "  (2, 14)\t0.26982521515445296\n",
      "  (2, 10)\t0.26982521515445296\n",
      "  (2, 8)\t0.26982521515445296\n",
      "  (2, 7)\t0.2052090309921715\n",
      "  (2, 5)\t0.26982521515445296\n",
      "  (2, 4)\t0.26982521515445296\n",
      "  (2, 0)\t0.26982521515445296\n"
     ]
    }
   ],
   "source": [
    "# text_spanish no tiene stopwords\n",
    "x_transformed = tfidf_transformer.fit_transform(text_spanish)\n",
    "print(x_transformed)\n",
    "#Abcdario =27 letras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar los lexemas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general siempre se prefiere la lematización, puesto que es un buen compromiso entre reducir la cantidad de tokens y preservar un poco más la composición original de estos. El stemming al ser más agresivo tiende a conllevar una pérdida de información más grande."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://josearcosaneas.github.io/python/r/procesamiento/lenguaje/2017/01/02/procesamiento-lenguaje-natural-0.html#:~:text=El%20stemming%20consiste%20en%20extreaer,ayuda%20de%20la%20librer%C3%ADa%20NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://es.linkedin.com/learning/python-para-data-scientist-avanzado/stemming-procesamiento-textual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ya sepas cómo eliminar todo el ruido más superficial de un texto como, por ejemplo, las \"stop words\", aquellas palabras que sencillamente no aportan información porque son conjunciones o preposiciones, y todos los signos de puntuación, el siguiente paso normalmente en un análisis de texto es utilizar un \"stemmer\". ¿Qué es exactamente un \"stemmer\"? Aquí te propongo usar este, que forma parte del NLTK. Y lo que estamos haciendo cuando usamos una función de este estilo es reducir las palabras a la raíz. Vamos a ver qué es lo que estamos haciendo. Aquí tenemos las palabras sin procesar, y una de las cosas que podemos hacer es reducir todas las conjugaciones del verbo \"llegar\" a su raíz. Es decir, llevar todas las palabras que sean \"llegaba\", \"llegaban\", etc., a \"llegar\". Esto es lo que hace una función de este estilo. Tenemos que inicializar 'stemmer' en el idioma que lo queremos. Hay bastantes más. El tema es que muchos de ellos no aceptan el español, así que yo recomiendo este si estás trabajando con texto en castellano, pero sabed que existe, por ejemplo, también el PorterStemmer, que también es muy potente. Aquí creamos el objeto. Y lo que vamos a hacer es, usando este \"loop\", coger cada una de las palabras que tengamos en la lista y vamos a extraer su raíz. He creado una lista de raíces, de momento vacía. Y vamos a extraer la raíz de cada una de ellas. Como puedes ver, es inmediato. Tenemos exactamente el mismo texto, pero hemos despojado a todas las palabras de sus sufijos, es decir, tenemos la raíz. Antes he hablado de \"llegar\" y ahora solo tenemos la raíz. Esto lo que permite es que, cuando hagamos un recuento de frecuencias, no nos veamos afectados por qué conjugación o qué variante de la palabra estemos usando. Si lo que queremos es usar la función 'FreqDist', de las raíces, cogemos, por ejemplo, las 20 primeras. Vemos efectivamente que no son las palabras en realidad, aunque hay algunas que sí. Señor, por ejemplo, tiene el mismo formato. Pero si este objeto encuentra \"señora\", también lo va a reducir a \"señor\". Este es el punto. Y comparamos esto con... Y podemos ver, por ejemplo, que \"caballero\" ahora es la palabra que más aparece, o la raíz asociada a \"caballero\", y aquí no lo era, estaba empatada con \"nombre\". Este proceso que, como has visto, es increíblemente sencillo, consta de crear un objeto y recorrer mediante un bucle todas las palabras que tenemos en nuestra lista. Recomiendo que primero lo limpies con una instrucción de este tipo. Y creamos una transformación de nuestras palabras que permite realizar un análisis mucho más específico que no dependa tanto de las circunstancias concretas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizar:\n",
    "- otra forma:\n",
    "tokens = [word for word in wordpunct_tokenize(texto_e)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deseo', 'dia', 'san', 'valentin', 'increible', 'amor', 'sueña', 'futuro', 'lado', 'sigamos', 'avanzando', 'trabajando', 'hacer', 'sueños', 'realidad', 'quede', 'corazones', 'feliz', 'dia', 'san', 'valentin', 'vida', 'quiero', 'sepas', 'cuanto', 'valoro', 'gran', 'lugar', 'ocupas', 'corazon', 'amo', 'locura']\n"
     ]
    }
   ],
   "source": [
    "print(lista_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducir las palabras a lexema y ver su frecuencia (cuantas veces aparecen):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primera prueba para entenderlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce palabras a su raíz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deseo:deseo\n",
      "dia:dia\n",
      "san:san\n",
      "valentin:valentin\n",
      "increible:increibl\n",
      "amor:amor\n",
      "sueña:sueña\n",
      "futuro:futuro\n",
      "lado:lado\n",
      "sigamos:sigamo\n",
      "avanzando:avanzando\n",
      "trabajando:trabajando\n",
      "hacer:hacer\n",
      "sueños:sueño\n",
      "realidad:realidad\n",
      "quede:qued\n",
      "corazones:corazon\n",
      "feliz:feliz\n",
      "dia:dia\n",
      "san:san\n",
      "valentin:valentin\n",
      "vida:vida\n",
      "quiero:quiero\n",
      "sepas:sepa\n",
      "cuanto:cuanto\n",
      "valoro:valoro\n",
      "gran:gran\n",
      "lugar:lugar\n",
      "ocupas:ocupa\n",
      "corazon:corazon\n",
      "amo:amo\n",
      "locura:locura\n",
      "['deseo', 'dia', 'san', 'valentin', 'increible', 'amor', 'sueña', 'futuro', 'lado', 'sigamos', 'avanzando', 'trabajando', 'hacer', 'sueños', 'realidad', 'quede', 'corazones', 'feliz', 'dia', 'san', 'valentin', 'vida', 'quiero', 'sepas', 'cuanto', 'valoro', 'gran', 'lugar', 'ocupas', 'corazon', 'amo', 'locura']\n",
      "['deseo', 'dia', 'san', 'valentin', 'increibl', 'amor', 'sueña', 'futuro', 'lado', 'sigamo', 'avanzando', 'trabajando', 'hacer', 'sueño', 'realidad', 'qued', 'corazon', 'feliz', 'dia', 'san', 'valentin', 'vida', 'quiero', 'sepa', 'cuanto', 'valoro', 'gran', 'lugar', 'ocupa', 'corazon', 'amo', 'locura']\n"
     ]
    }
   ],
   "source": [
    "tx_es_new = []\n",
    "num = 0\n",
    "tx_es = [p for p in lista_e if not p in set(spanish_stopwords)]\n",
    "for p in tx_es:\n",
    "    print(p + \":\" + ps.stem(p))\n",
    "    tx_es_new.append(ps.stem(p))\n",
    "    num = num + 1\n",
    "\n",
    "print(tx_es)\n",
    "print(tx_es_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "stemmers = [porter_stemmer.stem(word) for word in lista_e]\n",
    "final = [stem for stem in stemmers if stem.isalpha() and len(stem) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dia', 2), ('san', 2), ('valentin', 2), ('corazon', 2), ('deseo', 1), ('increibl', 1), ('amor', 1), ('sueña', 1), ('futuro', 1), ('lado', 1)]\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(final)\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo Porter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "sSnowball_stemmer = SnowballStemmer('spanish')\n",
    "stemmers2 = [sSnowball_stemmer.stem(word) for word in lista_e]\n",
    "final2 = [stem for stem in stemmers2 if stem.isalpha() and len(stem) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dia', 2), ('san', 2), ('valentin', 2), ('sueñ', 2), ('corazon', 2), ('dese', 1), ('increibl', 1), ('amor', 1), ('futur', 1), ('lad', 1)]\n"
     ]
    }
   ],
   "source": [
    "fdist2 = nltk.FreqDist(final2)\n",
    "print(fdist2.most_common(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keepcoding.io/blog/que-es-la-lematizacion-en-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lematización agrega contexto a las palabras, es preferible al stemming.\n",
    "\n",
    "Es una etapa importante en el PLN, está técnica algorítmica consiste en\n",
    "encontrar la forma morfológica básica de una palabra (lema), para que\n",
    "esto suceda se agrupan las palabras flexionadas que vendrían siendo\n",
    "verbos conjugados como: limpiar – limpiando, palabras en distintas\n",
    "formas como: gato – gata, entre otros, y se analiza como un solo elemento\n",
    "para encontrar la palabra base (Dereza, 2018) . "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stanza:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://es.stackoverflow.com/questions/341761/la-lematizaci%C3%B3n-de-spacy-en-castellano-no-es-muy-buena-conoc%C3%A9is-otros-paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3519916045.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[399], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    stanza.\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# stanza.download('es')\n",
    "stanza.\n",
    "nlp = stanza.Pipeline('es')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deseo', 'dia', 'san', 'valentin', 'increible', 'amor', 'sueña', 'futuro', 'lado', 'sigamos', 'avanzando', 'trabajando', 'hacer', 'sueños', 'realidad', 'quede', 'corazones', 'feliz', 'dia', 'san', 'valentin', 'vida', 'quiero', 'sepas', 'cuanto', 'valoro', 'gran', 'lugar', 'ocupas', 'corazon', 'amo', 'locura']\n",
      "deseo               deseo               \n",
      "dia                 dia                 \n",
      "san                 san                 \n",
      "valentin            valentin            \n",
      "increible           increible           \n",
      "amor                amor                \n",
      "sueña               sueña               \n",
      "futuro              futuro              \n",
      "lado                lado                \n",
      "sigamos             sigamos             \n",
      "avanzando           avanzando           \n",
      "trabajando          trabajando          \n",
      "hacer               hacer               \n",
      "sueños              sueños              \n",
      "realidad            realidad            \n",
      "quede               quede               \n",
      "corazones           corazones           \n",
      "feliz               feliz               \n",
      "dia                 dia                 \n",
      "san                 san                 \n",
      "valentin            valentin            \n",
      "vida                vida                \n",
      "quiero              quiero              \n",
      "sepas               sepas               \n",
      "cuanto              cuanto              \n",
      "valoro              valoro              \n",
      "gran                gran                \n",
      "lugar               lugar               \n",
      "ocupas              ocupas              \n",
      "corazon             corazon             \n",
      "amo                 amo                 \n",
      "locura              locura              \n"
     ]
    }
   ],
   "source": [
    "print(lista_e)\n",
    "for p in lista_e:\n",
    "    print(\"{0:20}{1:20}\".format(p, wordnet_lemmatizer.lemmatize(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deseo               deseo               \n",
      "dia                 dia                 \n",
      "san                 san                 \n",
      "valentin            valentin            \n",
      "increible           increible           \n",
      "amor                amor                \n",
      "sueña               sueña               \n",
      "futuro              futuro              \n",
      "lado                lado                \n",
      "sigamos             sigamos             \n",
      "avanzando           avanzando           \n",
      "trabajando          trabajando          \n",
      "hacer               hacer               \n",
      "sueños              sueños              \n",
      "realidad            realidad            \n",
      "quede               quede               \n",
      "corazones           corazones           \n",
      "feliz               feliz               \n",
      "dia                 dia                 \n",
      "san                 san                 \n",
      "valentin            valentin            \n",
      "vida                vida                \n",
      "quiero              quiero              \n",
      "sepas               sepas               \n",
      "cuanto              cuanto              \n",
      "valoro              valoro              \n",
      "gran                gran                \n",
      "lugar               lugar               \n",
      "ocupas              ocupas              \n",
      "corazon             corazon             \n",
      "amo                 amo                 \n",
      "locura              locura              \n"
     ]
    }
   ],
   "source": [
    "for p in lista_e:\n",
    "    print(\"{0:20}{1:20}\".format(p, wordnet_lemmatizer.lemmatize(p, pos='v')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__cinit__() takes exactly 3 positional arguments (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[218], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokens\u001b[39;00m \u001b[39mimport\u001b[39;00m Token\n\u001b[1;32m----> 2\u001b[0m y \u001b[39m=\u001b[39m Token()\n\u001b[0;32m      3\u001b[0m \u001b[39mset\u001b[39m((y\u001b[39m.\u001b[39mlemma_\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m lista_e))\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\tokens\\token.pyx:87\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __cinit__() takes exactly 3 positional arguments (0 given)"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "y = Token()\n",
    "set((y.lemma_.lower() for token in lista_e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "\n",
    "#nlp = spacy.load(\"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy \n",
    "from spacy.lang.en import English\n",
    "# nlp = spacy.load(es_core_news_sm)\n",
    "# doc = English(str_i) # Crea un objeto de spacy tipo nlp\n",
    "# tokens = [t.orth_ for t in doc] # Crea una lista con las palabras del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = normalize(str_i) # crear una lista de tokens\n",
    "# stems = [spanishstemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbol de decision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cienciadedatos.net/documentos/py07_arboles_decision_python.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gráficos\n",
    "# ------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ------------------------------------------------------------------------------\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[351], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m carseats \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mget_rdataset(\u001b[39m\"\u001b[39m\u001b[39mCarseats\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mISLR\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m datos \u001b[39m=\u001b[39m carseats\u001b[39m.\u001b[39mdata\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(carseats\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sm' is not defined"
     ]
    }
   ],
   "source": [
    "carseats = sm.datasets.get_rdataset(\"Carseats\", \"ISLR\")\n",
    "datos = carseats.data\n",
    "print(carseats.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matriz de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [\"gato\", \"lobo\", \"gato\", \"gato\", \"lobo\", \"perro\"] # i => Renglones\n",
    "y_pred = [\"lobo\", \"lobo\", \"gato\", \"gato\", \"lobo\", \"gato\"]  # j => Columnas\n",
    "confusion_matrix(y_true, y_pred, labels=[\"lobo\", \"perro\", \"gato\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalize = true =  1.0\n",
      "normalize = false =  4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 1, 2, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    " \n",
    "print(\"normalize = true = \", accuracy_score(y_true, y_pred))\n",
    "print(\"normalize = false = \", accuracy_score(y_true, y_pred, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "data.target[[10, 25, 50]]\n",
    " \n",
    "list(data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "y_true = [0, 1, 0, 0, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 0, 1]\n",
    "precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tflearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pycodemates.com/2021/11/build-a-AI-chatbot-using-python-and-deep-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Either a `shape` or `placeholder` argument is required to consruct an input layer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[406], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m tflearn\u001b[39m.\u001b[39;49minput_data()\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tflearn\\layers\\core.py:69\u001b[0m, in \u001b[0;36minput_data\u001b[1;34m(shape, placeholder, dtype, data_preprocessing, data_augmentation, name)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m placeholder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEither a `shape` or `placeholder` argument is required to consruct an input layer.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m     \u001b[39m# We have a shape but no placeholder, so we must now create a placeholder.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m     \u001b[39m# Ensure the first element of shape is None by prepending None if necessary.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[39m# TODO: Why is there a len(shape)>1 condition? Please explain here.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shape) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m shape[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mException\u001b[0m: Either a `shape` or `placeholder` argument is required to consruct an input layer."
     ]
    }
   ],
   "source": [
    "model = tflearn.input_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[410], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m net \u001b[39m=\u001b[39m tflearn\u001b[39m.\u001b[39mregression(net)\n\u001b[0;32m      8\u001b[0m model \u001b[39m=\u001b[39m tflearn\u001b[39m.\u001b[39mDNN(net)\n\u001b[1;32m----> 9\u001b[0m model\u001b[39m.\u001b[39;49mfit(t_e, respuesta_e, n_epoch\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, show_metric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     10\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mmodel.tflearn\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tflearn\\models\\dnn.py:183\u001b[0m, in \u001b[0;36mDNN.fit\u001b[1;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[0;32m    178\u001b[0m         valY \u001b[39m=\u001b[39m validation_set[\u001b[39m1\u001b[39m]\n\u001b[0;32m    180\u001b[0m \u001b[39m# For simplicity we build sync dict synchronously but Trainer support\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m# asynchronous feed dict allocation.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m# TODO: check memory impact for large data and multiple optimizers\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m feed_dict \u001b[39m=\u001b[39m feed_dict_builder(X_inputs, Y_targets, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minputs,\n\u001b[0;32m    184\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets)\n\u001b[0;32m    185\u001b[0m feed_dicts \u001b[39m=\u001b[39m [feed_dict \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_ops]\n\u001b[0;32m    186\u001b[0m val_feed_dicts \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tflearn\\utils.py:300\u001b[0m, in \u001b[0;36mfeed_dict_builder\u001b[1;34m(X, Y, net_inputs, net_targets)\u001b[0m\n\u001b[0;32m    298\u001b[0m         X \u001b[39m=\u001b[39m [X]\n\u001b[0;32m    299\u001b[0m     \u001b[39mfor\u001b[39;00m i, x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(X):\n\u001b[1;32m--> 300\u001b[0m         feed_dict[net_inputs[i]] \u001b[39m=\u001b[39m x\n\u001b[0;32m    301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m     \u001b[39m# If a dict is provided\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m X\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    304\u001b[0m         \u001b[39m# Copy to feed_dict if dict already fits {placeholder: data} template\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "net = tflearn.input_data(shape=[None, len(t_e[0])])\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, len(respuesta_e[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "model.fit(t_e, respuesta_e, n_epoch=500, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_llamado_svm = svm.SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_llamado_svm.fit(x_transformed, respuesta_e) #x = x_transformer, y = respuesta_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[299], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m matriz \u001b[39m=\u001b[39m (x_transformed, respuesta_e)\n\u001b[1;32m----> 2\u001b[0m prediction \u001b[39m=\u001b[39m algo_llamado_svm\u001b[39m.\u001b[39;49mpredict(matriz) \u001b[39m#acurracy test\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:820\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    818\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    819\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    821\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp))\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:433\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \n\u001b[0;32m    420\u001b[0m \u001b[39m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39m        The predicted values.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_for_predict(X)\n\u001b[0;32m    434\u001b[0m     predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_predict \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_predict\n\u001b[0;32m    435\u001b[0m     \u001b[39mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:613\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    610\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    612\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel):\n\u001b[1;32m--> 613\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    614\u001b[0m         X,\n\u001b[0;32m    615\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    616\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[0;32m    617\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    618\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    619\u001b[0m         reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sp\u001b[39m.\u001b[39misspmatrix(X):\n\u001b[0;32m    623\u001b[0m     X \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "matriz = (x_transformed, respuesta_e)\n",
    "prediction = algo_llamado_svm.predict(matriz) #acurracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0%\n"
     ]
    }
   ],
   "source": [
    "print (u'Accuracy:  {:.1%}'.format(np.mean(prediction == respuesta_e)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arnaudunjo.com/es/2021/04/25/machine-learning-modelo-clasificador-de-textos-en-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[306], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msvm\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearSVC\n\u001b[0;32m      3\u001b[0m model \u001b[39m=\u001b[39m LinearSVC()\n\u001b[1;32m----> 5\u001b[0m X_train, X_test, y_train, y_test, indices_train, indices_test \u001b[39m=\u001b[39m train_test_split(features, labels, df\u001b[39m.\u001b[39mindex, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      7\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.2, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatterbbot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=Yd0e4uven-g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://realpython.com/build-a-chatbot-python-chatterbot/#step-1-create-a-chatbot-using-python-chatterbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatterbot import ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'time' has no attribute 'clock'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[350], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mchatterbot\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainers\u001b[39;00m \u001b[39mimport\u001b[39;00m ListTrainer\n\u001b[1;32m----> 2\u001b[0m chatbot \u001b[39m=\u001b[39m ChatBot(\u001b[39m\"\u001b[39;49m\u001b[39mchat\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m trainer \u001b[39m=\u001b[39m ListTrainer(chatbot)\n\u001b[0;32m      5\u001b[0m trainer\u001b[39m.\u001b[39mtrain([\n\u001b[0;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHi\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mWelcome, friend 🤗\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m ])\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\chatterbot\\chatterbot.py:34\u001b[0m, in \u001b[0;36mChatBot.__init__\u001b[1;34m(self, name, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m# Logic adapters used by the chat bot\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogic_adapters \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstorage \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49minitialize_class(storage_adapter, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m adapter \u001b[39min\u001b[39;00m logic_adapters:\n\u001b[0;32m     37\u001b[0m     utils\u001b[39m.\u001b[39mvalidate_adapter_class(adapter, LogicAdapter)\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\chatterbot\\utils.py:54\u001b[0m, in \u001b[0;36minitialize_class\u001b[1;34m(data, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     Class \u001b[39m=\u001b[39m import_module(data)\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m Class(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\chatterbot\\storage\\sql_storage.py:22\u001b[0m, in \u001b[0;36mSQLStorageAdapter.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     20\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msqlalchemy\u001b[39;00m \u001b[39mimport\u001b[39;00m create_engine\n\u001b[0;32m     23\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msqlalchemy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39morm\u001b[39;00m \u001b[39mimport\u001b[39;00m sessionmaker\n\u001b[0;32m     25\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatabase_uri \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdatabase_uri\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# sqlalchemy/__init__.py\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Copyright (C) 2005-2019 the SQLAlchemy authors and contributors\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# <see AUTHORS file>\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# This module is part of SQLAlchemy and is released under\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# the MIT License: http://www.opensource.org/licenses/mit-license.php\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m util \u001b[39mas\u001b[39;00m _util  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minspection\u001b[39;00m \u001b[39mimport\u001b[39;00m inspect  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m \u001b[39mimport\u001b[39;00m BLANK_SCHEMA  \u001b[39m# noqa\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\util\\__init__.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m update_wrapper  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_collections\u001b[39;00m \u001b[39mimport\u001b[39;00m coerce_generator_arg  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_collections\u001b[39;00m \u001b[39mimport\u001b[39;00m collections_abc  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_collections\u001b[39;00m \u001b[39mimport\u001b[39;00m column_dict  \u001b[39m# noqa\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\util\\_collections.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mweakref\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m binary_types\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m collections_abc\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m itertools_filterfalse\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:264\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39minspect\u001b[39;00m \u001b[39mimport\u001b[39;00m formatargspec \u001b[39mas\u001b[39;00m inspect_formatargspec  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39mif\u001b[39;00m win32 \u001b[39mor\u001b[39;00m jython:\n\u001b[1;32m--> 264\u001b[0m     time_func \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39;49mclock\n\u001b[0;32m    265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     time_func \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'time' has no attribute 'clock'"
     ]
    }
   ],
   "source": [
    "from chatterbot.trainers import ListTrainer\n",
    "chatbot = ChatBot(\"chat\")\n",
    "\n",
    "trainer = ListTrainer(chatbot)\n",
    "trainer.train([\n",
    "    \"Hi\",\n",
    "    \"Welcome, friend 🤗\",\n",
    "])\n",
    "trainer.train([\n",
    "    \"Are you a plant?\",\n",
    "    \"No, I'm the pot below the plant!\",\n",
    "])\n",
    "\n",
    "exit_conditions = (\":q\", \"quit\", \"exit\")\n",
    "while True:\n",
    "    query = input(\"> \")\n",
    "    if query in exit_conditions:\n",
    "        break\n",
    "    else:\n",
    "        print(f\"🪴 {chatbot.get_response(query)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secuential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = keras.models.Sequential() # Pag 532 cap 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirar keras.backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrevista de trabajo:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/discussions/general/241749"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opiniones de tripadvisor:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/rtatman/deceptive-opinion-spam-corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis sentimientos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://unipython.com/proyecto-desarrollar-un-modelo-neural-de-bolsa-de-palabras-para-el-analisis-de-sentimientos/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/qu4nt/reducir-el-n%C3%BAmero-de-palabras-de-un-texto-lematizaci%C3%B3n-y-radicalizaci%C3%B3n-stemming-con-python-965bfd0c69fa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://josearcosaneas.github.io/python/r/procesamiento/lenguaje/2017/01/02/procesamiento-lenguaje-natural-0.html#:~:text=El%20stemming%20consiste%20en%20extreaer,ayuda%20de%20la%20librer%C3%ADa%20NLTK."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentación Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/text/tutorials/nmt_with_attention?hl=es-419"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de chatbot:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://planetachatbot.com/tutorial-chatbot-usando-nltk-keras/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/api/utils/backend_utils/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://faroit.com/keras-docs/1.2.0/backend/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyectos parecidos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dspace.ups.edu.ec/bitstream/123456789/21195/1/UPS-CT009315.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://dspace.ups.edu.ec/bitstream/123456789/22069/1/UPS-CT009621.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://digital.csic.es/bitstream/10261/227679/1/TFM_VeronicaPinillaGomez%20%281%29.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://m.riunet.upv.es/bitstream/handle/10251/128834/Gallardo%20-%20Creaci%C3%B3n%20de%20un%20modelo%20predictivo%20para%20clasificar%20las%20consultas%20ciudadanas%20sobre%20el%20tra....pdf?sequence=1&isAllowed=y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ce52cbbbc6e028708724a15cf5ae56ba876571c2d68b4c149d42b015a691c4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
