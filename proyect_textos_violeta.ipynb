{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frases empalagosas San Valentin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports y downloads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports (modulos usados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import svm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"popular\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación, preprocesamiento y análisis de datos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de los datos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizar: Crear una lista de palablas (Tokens) a partir de un texto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Español:\n",
    "- str_e    ->  String español \n",
    "- lista_e  ->  Lista tokens español\n",
    "\n",
    "Ingles:\n",
    "- str_i    ->  String español \n",
    "- lista_e  ->  Lista tokens español"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto español"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_e = [[\"¡Te deseo un día de San Valentín increíble, mi amor! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad\"], \n",
    "[\"¡Feliz día de San Valentín, mi vida! quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón. Te amo con locura\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_e2 = [\"Ten un día de San Valentín estupendo, sigamosconstruyendo nuestro amor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_e = [\"Muchas gracias, yo a tí tambien\", \"¿que has dicho?\", \"felicidades a tí también\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡Te deseo un día de San Valentín increíble, mi amor! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad', ' ', '¡Feliz día de San Valentín, mi vida! quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón. Te amo con locura']\n"
     ]
    }
   ],
   "source": [
    "t_e = []\n",
    "\n",
    "for i in range(0, len(texto_e)):\n",
    "    if(i>0):\n",
    "        t_e =t_e + [\" \"]\n",
    "    t_e = t_e + texto_e[i]\n",
    "\n",
    "print(t_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Te deseo un día de San Valentín increíble, mi amor! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad ¡Feliz día de San Valentín, mi vida! quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón. Te amo con locura\n"
     ]
    }
   ],
   "source": [
    "str_e = \"\"\n",
    "\n",
    "for i in range(0, len(t_e)):\n",
    "    str_e = str_e + str(t_e[i])\n",
    "print(str_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://micro.recursospython.com/recursos/como-quitar-tildes-de-una-cadena.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quitar signos de puntuacion y dejar las letras en minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te deseo un día de san valentín increíble mi amor sigamos avanzando y trabajando por hacer de nuestros sueños una realidad feliz día de san valentín mi vida quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón te amo con locura\n"
     ]
    }
   ],
   "source": [
    "# También se puede usar esto\n",
    "import string\n",
    "string.punctuation\n",
    "str_aux = \"\"\n",
    " # end used to print the output in one line\n",
    "for i in str_e:\n",
    "    if i in (string.punctuation+\"¡¿\"):\n",
    "        i = \"\" \n",
    "    str_aux = str_aux + i.lower()           # checking for occurence of any punctuation\n",
    "str_e = str_aux\n",
    "print(str_e)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "spanish_stopwords = stopwords.words('spanish')\n",
    "print(spanish_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deseo día san valentín increíble amor sigamos avanzando trabajando hacer sueños realidad feliz día san valentín vida quiero sepas cuanto valoro gran lugar ocupas corazón amo locura\n"
     ]
    }
   ],
   "source": [
    "str_e = \" \" + str_e + \" \"\n",
    "for i in range(0, len(spanish_stopwords)):\n",
    "    if \" \" + spanish_stopwords[i] + \" \" in str_e:\n",
    "        str_e = str_e.replace(\" \"+ spanish_stopwords[i] + \" \", \" \")\n",
    "str_e = str_e.strip()\n",
    "print(str_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quitar tildes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deseo dia san valentin increible amor sigamos avanzando trabajando hacer sueños realidad feliz dia san valentin vida quiero sepas cuanto valoro gran lugar ocupas corazon amo locura\n"
     ]
    }
   ],
   "source": [
    "tildes = (\n",
    "    (\"á\", \"a\"),\n",
    "    (\"é\", \"e\"),\n",
    "    (\"í\", \"i\"),\n",
    "    (\"ó\", \"o\"),\n",
    "    (\"ú\", \"u\"),\n",
    ")\n",
    "for letra_tilde, letra in tildes:\n",
    "    str_e = str_e.replace(letra_tilde, letra).replace(letra.upper(), letra_tilde.upper())\n",
    "\n",
    "print(str_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deseo', 'dia', 'san', 'valentin', 'increible', 'amor', 'sigamos', 'avanzando', 'trabajando', 'hacer', 'sueños', 'realidad', 'feliz', 'dia', 'san', 'valentin', 'vida', 'quiero', 'sepas', 'cuanto', 'valoro', 'gran', 'lugar', 'ocupas', 'corazon', 'amo', 'locura']\n"
     ]
    }
   ],
   "source": [
    "lista_e = str_e.split()\n",
    "print(lista_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NO ME SALIÓ : Tokenizar + quitar signos de puntuación (no me lo ví bien deja espacios en blanco porque sí)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import re\\n# cargando el texto\\nlista_e = re.split(r'\\\\W+', str_e.lower())\\nprint(lista_e)\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import re\n",
    "# cargando el texto\n",
    "lista_e = re.split(r'\\W+', str_e.lower())\n",
    "print(lista_e)\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto ingles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear texto en ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_i = [[\"I love you.\"], [\"A hundred hearts would be too few to carry all my love for you.\"], [\"If you were thinking about someone while reading this, you're definitely in love.\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love you.', ' ', 'A hundred hearts would be too few to carry all my love for you.', ' ', \"If you were thinking about someone while reading this, you're definitely in love.\"]\n"
     ]
    }
   ],
   "source": [
    "t_i = []\n",
    "for i in range(0, len(texto_i)):\n",
    "    if(i>0):\n",
    "        t_i =t_i + [\" \"]\n",
    "    t_i = t_i + texto_i[i]\n",
    "\n",
    "print(t_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = stopwords.words('english')\n",
    "print(english_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear string en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love you. A hundred hearts would be too few to carry all my love for you. If you were thinking about someone while reading this, you're definitely in love.\n"
     ]
    }
   ],
   "source": [
    "str_i = \"\"\n",
    "for i in range(0, len(t_i)):\n",
    "    str_i = str_i + str(t_i[i])\n",
    "print(str_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quitar signos de puntuacion y dejar las letras en minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you a hundred hearts would be too few to carry all my love for you if you were thinking about someone while reading this youre definitely in love\n"
     ]
    }
   ],
   "source": [
    "# También se puede usar esto\n",
    "import string\n",
    "string.punctuation\n",
    "str_aux = \"\"\n",
    " # end used to print the output in one line\n",
    "for i in str_i:\n",
    "    if i in (string.punctuation+\"¡\"):\n",
    "        i = \"\" \n",
    "    str_aux = str_aux + i.lower()           # checking for occurence of any punctuation\n",
    "str_i = str_aux\n",
    "print(str_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'you', 'a', 'hundred', 'hearts', 'would', 'be', 'too', 'few', 'to', 'carry', 'all', 'my', 'love', 'for', 'you', 'if', 'you', 'were', 'thinking', 'about', 'someone', 'while', 'reading', 'this', 'youre', 'definitely', 'in', 'love']\n"
     ]
    }
   ],
   "source": [
    "lista_i = str_i.split()\n",
    "print(lista_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NO ME SALIÓ : Tokenizar + quitar signos de puntuación (no me lo ví bien deja espacios en blanco porque sí)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import re\\n# cargando el texto\\nlista_i = re.split(r'\\\\W+', str_i.lower())\\nprint(lista_i)\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import re\n",
    "# cargando el texto\n",
    "lista_i = re.split(r'\\W+', str_i.lower())\n",
    "print(lista_i)\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniza un texto a través de la creación de un vocabulario con las palabras que éste contiene.\n",
    "<br><br>\n",
    "Realiza una transformación cuyo objetivo es obtener un mejor tratamiento de los datos.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texto = [[\"¡Te deseo un día de San Valentín increíble, mi amor! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad\"], [\"\"]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de palabras "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista con el texto a normalizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡Te deseo un día de San Valentín increíble, mi amor! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad', ' ', '¡Feliz día de San Valentín, mi vida! quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón. Te amo con locura']\n"
     ]
    }
   ],
   "source": [
    "print(t_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el count_vectorizer con las stopwords españolas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=spanish_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_spanish = count_vectorizer.fit_transform(t_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(text_spanish.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amo', 'amor', 'avanzando', 'corazón', 'cuanto', 'deseo', 'día',\n",
       "       'feliz', 'gran', 'hacer', 'increíble', 'locura', 'lugar', 'ocupas',\n",
       "       'quiero', 'realidad', 'san', 'sepas', 'sigamos', 'sueños',\n",
       "       'trabajando', 'valentín', 'valoro', 'vida'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx1 = count_vectorizer.fit(t_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.educative.io/answers/countvectorizer-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deseo': 5, 'día': 6, 'san': 16, 'valentín': 21, 'increíble': 10, 'amor': 1, 'sigamos': 18, 'avanzando': 2, 'trabajando': 20, 'hacer': 9, 'sueños': 19, 'realidad': 15, 'feliz': 7, 'vida': 23, 'quiero': 14, 'sepas': 17, 'cuanto': 4, 'valoro': 22, 'gran': 8, 'lugar': 12, 'ocupas': 13, 'corazón': 3, 'amo': 0, 'locura': 11}\n"
     ]
    }
   ],
   "source": [
    "print(tx1.vocabulary_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es para saber si una palabra del array mostrado en la anterior está presente en esa frase:\n",
    "- 0 = no está\n",
    "- 1 = está"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(text_spanish.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 24)\n"
     ]
    }
   ],
   "source": [
    "print(text_spanish.shape) # Muestra las dimensiones de la variable vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21)\t0.23211804319157203\n",
      "  (0, 20)\t0.30520733245792603\n",
      "  (0, 19)\t0.30520733245792603\n",
      "  (0, 18)\t0.30520733245792603\n",
      "  (0, 16)\t0.23211804319157203\n",
      "  (0, 15)\t0.30520733245792603\n",
      "  (0, 10)\t0.30520733245792603\n",
      "  (0, 9)\t0.30520733245792603\n",
      "  (0, 6)\t0.23211804319157203\n",
      "  (0, 5)\t0.30520733245792603\n",
      "  (0, 2)\t0.30520733245792603\n",
      "  (0, 1)\t0.30520733245792603\n",
      "  (2, 23)\t0.26982521515445296\n",
      "  (2, 22)\t0.26982521515445296\n",
      "  (2, 21)\t0.2052090309921715\n",
      "  (2, 17)\t0.26982521515445296\n",
      "  (2, 16)\t0.2052090309921715\n",
      "  (2, 14)\t0.26982521515445296\n",
      "  (2, 13)\t0.26982521515445296\n",
      "  (2, 12)\t0.26982521515445296\n",
      "  (2, 11)\t0.26982521515445296\n",
      "  (2, 8)\t0.26982521515445296\n",
      "  (2, 7)\t0.26982521515445296\n",
      "  (2, 6)\t0.2052090309921715\n",
      "  (2, 4)\t0.26982521515445296\n",
      "  (2, 3)\t0.26982521515445296\n",
      "  (2, 0)\t0.26982521515445296\n"
     ]
    }
   ],
   "source": [
    "# text_spanish no tiene stopwords\n",
    "x_transformed = tfidf_transformer.fit_transform(text_spanish)\n",
    "print(x_transformed)\n",
    "#Abcdario =27 letras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar los lexemas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general siempre se prefiere la lematización, puesto que es un buen compromiso entre reducir la cantidad de tokens y preservar un poco más la composición original de estos. El stemming al ser más agresivo tiende a conllevar una pérdida de información más grande."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://josearcosaneas.github.io/python/r/procesamiento/lenguaje/2017/01/02/procesamiento-lenguaje-natural-0.html#:~:text=El%20stemming%20consiste%20en%20extreaer,ayuda%20de%20la%20librer%C3%ADa%20NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://es.linkedin.com/learning/python-para-data-scientist-avanzado/stemming-procesamiento-textual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ya sepas cómo eliminar todo el ruido más superficial de un texto como, por ejemplo, las \"stop words\", aquellas palabras que sencillamente no aportan información porque son conjunciones o preposiciones, y todos los signos de puntuación, el siguiente paso normalmente en un análisis de texto es utilizar un \"stemmer\". ¿Qué es exactamente un \"stemmer\"? Aquí te propongo usar este, que forma parte del NLTK. Y lo que estamos haciendo cuando usamos una función de este estilo es reducir las palabras a la raíz. Vamos a ver qué es lo que estamos haciendo. Aquí tenemos las palabras sin procesar, y una de las cosas que podemos hacer es reducir todas las conjugaciones del verbo \"llegar\" a su raíz. Es decir, llevar todas las palabras que sean \"llegaba\", \"llegaban\", etc., a \"llegar\". Esto es lo que hace una función de este estilo. Tenemos que inicializar 'stemmer' en el idioma que lo queremos. Hay bastantes más. El tema es que muchos de ellos no aceptan el español, así que yo recomiendo este si estás trabajando con texto en castellano, pero sabed que existe, por ejemplo, también el PorterStemmer, que también es muy potente. Aquí creamos el objeto. Y lo que vamos a hacer es, usando este \"loop\", coger cada una de las palabras que tengamos en la lista y vamos a extraer su raíz. He creado una lista de raíces, de momento vacía. Y vamos a extraer la raíz de cada una de ellas. Como puedes ver, es inmediato. Tenemos exactamente el mismo texto, pero hemos despojado a todas las palabras de sus sufijos, es decir, tenemos la raíz. Antes he hablado de \"llegar\" y ahora solo tenemos la raíz. Esto lo que permite es que, cuando hagamos un recuento de frecuencias, no nos veamos afectados por qué conjugación o qué variante de la palabra estemos usando. Si lo que queremos es usar la función 'FreqDist', de las raíces, cogemos, por ejemplo, las 20 primeras. Vemos efectivamente que no son las palabras en realidad, aunque hay algunas que sí. Señor, por ejemplo, tiene el mismo formato. Pero si este objeto encuentra \"señora\", también lo va a reducir a \"señor\". Este es el punto. Y comparamos esto con... Y podemos ver, por ejemplo, que \"caballero\" ahora es la palabra que más aparece, o la raíz asociada a \"caballero\", y aquí no lo era, estaba empatada con \"nombre\". Este proceso que, como has visto, es increíblemente sencillo, consta de crear un objeto y recorrer mediante un bucle todas las palabras que tenemos en nuestra lista. Recomiendo que primero lo limpies con una instrucción de este tipo. Y creamos una transformación de nuestras palabras que permite realizar un análisis mucho más específico que no dependa tanto de las circunstancias concretas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizar:\n",
    "- otra forma:\n",
    "tokens = [word for word in wordpunct_tokenize(texto_e)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deseo', 'dia', 'san', 'valentin', 'increible', 'amor', 'sigamos', 'avanzando', 'trabajando', 'hacer', 'sueños', 'realidad', 'feliz', 'dia', 'san', 'valentin', 'vida', 'quiero', 'sepas', 'cuanto', 'valoro', 'gran', 'lugar', 'ocupas', 'corazon', 'amo', 'locura']\n"
     ]
    }
   ],
   "source": [
    "print(lista_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducir las palabras a lexema y ver su frecuencia (cuantas veces aparecen):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primera prueba para entenderlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce palabras a su raíz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deseo:deseo\n",
      "dia:dia\n",
      "san:san\n",
      "valentin:valentin\n",
      "increible:increibl\n",
      "amor:amor\n",
      "sigamos:sigamo\n",
      "avanzando:avanzando\n",
      "trabajando:trabajando\n",
      "hacer:hacer\n",
      "sueños:sueño\n",
      "realidad:realidad\n",
      "feliz:feliz\n",
      "dia:dia\n",
      "san:san\n",
      "valentin:valentin\n",
      "vida:vida\n",
      "quiero:quiero\n",
      "sepas:sepa\n",
      "cuanto:cuanto\n",
      "valoro:valoro\n",
      "gran:gran\n",
      "lugar:lugar\n",
      "ocupas:ocupa\n",
      "corazon:corazon\n",
      "amo:amo\n",
      "locura:locura\n",
      "['deseo', 'dia', 'san', 'valentin', 'increible', 'amor', 'sigamos', 'avanzando', 'trabajando', 'hacer', 'sueños', 'realidad', 'feliz', 'dia', 'san', 'valentin', 'vida', 'quiero', 'sepas', 'cuanto', 'valoro', 'gran', 'lugar', 'ocupas', 'corazon', 'amo', 'locura']\n",
      "['deseo', 'dia', 'san', 'valentin', 'increibl', 'amor', 'sigamo', 'avanzando', 'trabajando', 'hacer', 'sueño', 'realidad', 'feliz', 'dia', 'san', 'valentin', 'vida', 'quiero', 'sepa', 'cuanto', 'valoro', 'gran', 'lugar', 'ocupa', 'corazon', 'amo', 'locura']\n"
     ]
    }
   ],
   "source": [
    "tx_es_new = []\n",
    "num = 0\n",
    "tx_es = [p for p in lista_e if not p in set(spanish_stopwords)]\n",
    "for p in tx_es:\n",
    "    print(p + \":\" + ps.stem(p))\n",
    "    tx_es_new.append(ps.stem(p))\n",
    "    num = num + 1\n",
    "\n",
    "print(tx_es)\n",
    "print(tx_es_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "stemmers = [porter_stemmer.stem(word) for word in lista_e]\n",
    "final = [stem for stem in stemmers if stem.isalpha() and len(stem) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dia', 2), ('san', 2), ('valentin', 2), ('deseo', 1), ('increibl', 1), ('amor', 1), ('sigamo', 1), ('avanzando', 1), ('trabajando', 1), ('hacer', 1)]\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(final)\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo Porter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sSnowball_stemmer = SnowballStemmer('spanish')\n",
    "stemmers2 = [sSnowball_stemmer.stem(word) for word in lista_e]\n",
    "final2 = [stem for stem in stemmers2 if stem.isalpha() and len(stem) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dia', 2), ('san', 2), ('valentin', 2), ('dese', 1), ('increibl', 1), ('amor', 1), ('sig', 1), ('avanz', 1), ('trabaj', 1), ('hac', 1)]\n"
     ]
    }
   ],
   "source": [
    "fdist2 = nltk.FreqDist(final2)\n",
    "print(fdist2.most_common(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keepcoding.io/blog/que-es-la-lematizacion-en-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lematización agrega contexto a las palabras, es preferible al stemming.\n",
    "\n",
    "Es una etapa importante en el PLN, está técnica algorítmica consiste en\n",
    "encontrar la forma morfológica básica de una palabra (lema), para que\n",
    "esto suceda se agrupan las palabras flexionadas que vendrían siendo\n",
    "verbos conjugados como: limpiar – limpiando, palabras en distintas\n",
    "formas como: gato – gata, entre otros, y se analiza como un solo elemento\n",
    "para encontrar la palabra base (Dereza, 2018) . "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deseo', 'dia', 'san', 'valentin', 'increible', 'amor', 'sigamos', 'avanzando', 'trabajando', 'hacer', 'sueños', 'realidad', 'feliz', 'dia', 'san', 'valentin', 'vida', 'quiero', 'sepas', 'cuanto', 'valoro', 'gran', 'lugar', 'ocupas', 'corazon', 'amo', 'locura']\n",
      "deseo               deseo               \n",
      "dia                 dia                 \n",
      "san                 san                 \n",
      "valentin            valentin            \n",
      "increible           increible           \n",
      "amor                amor                \n",
      "sigamos             sigamos             \n",
      "avanzando           avanzando           \n",
      "trabajando          trabajando          \n",
      "hacer               hacer               \n",
      "sueños              sueños              \n",
      "realidad            realidad            \n",
      "feliz               feliz               \n",
      "dia                 dia                 \n",
      "san                 san                 \n",
      "valentin            valentin            \n",
      "vida                vida                \n",
      "quiero              quiero              \n",
      "sepas               sepas               \n",
      "cuanto              cuanto              \n",
      "valoro              valoro              \n",
      "gran                gran                \n",
      "lugar               lugar               \n",
      "ocupas              ocupas              \n",
      "corazon             corazon             \n",
      "amo                 amo                 \n",
      "locura              locura              \n"
     ]
    }
   ],
   "source": [
    "print(lista_e)\n",
    "for p in lista_e:\n",
    "    print(\"{0:20}{1:20}\".format(p, wordnet_lemmatizer.lemmatize(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deseo               deseo               \n",
      "dia                 dia                 \n",
      "san                 san                 \n",
      "valentin            valentin            \n",
      "increible           increible           \n",
      "amor                amor                \n",
      "sigamos             sigamos             \n",
      "avanzando           avanzando           \n",
      "trabajando          trabajando          \n",
      "hacer               hacer               \n",
      "sueños              sueños              \n",
      "realidad            realidad            \n",
      "feliz               feliz               \n",
      "dia                 dia                 \n",
      "san                 san                 \n",
      "valentin            valentin            \n",
      "vida                vida                \n",
      "quiero              quiero              \n",
      "sepas               sepas               \n",
      "cuanto              cuanto              \n",
      "valoro              valoro              \n",
      "gran                gran                \n",
      "lugar               lugar               \n",
      "ocupas              ocupas              \n",
      "corazon             corazon             \n",
      "amo                 amo                 \n",
      "locura              locura              \n"
     ]
    }
   ],
   "source": [
    "for p in lista_e:\n",
    "    print(\"{0:20}{1:20}\".format(p, wordnet_lemmatizer.lemmatize(p, pos='v')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__cinit__() takes exactly 3 positional arguments (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokens\u001b[39;00m \u001b[39mimport\u001b[39;00m Token\n\u001b[1;32m----> 2\u001b[0m y \u001b[39m=\u001b[39m Token()\n\u001b[0;32m      3\u001b[0m \u001b[39mset\u001b[39m((y\u001b[39m.\u001b[39mlemma_\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m lista_e))\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\tokens\\token.pyx:87\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __cinit__() takes exactly 3 positional arguments (0 given)"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "y = Token()\n",
    "set((y.lemma_.lower() for token in lista_e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "\n",
    "#nlp = spacy.load(\"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy \n",
    "from spacy.lang.en import English\n",
    "# nlp = spacy.load(es_core_news_sm)\n",
    "# doc = English(str_i) # Crea un objeto de spacy tipo nlp\n",
    "# tokens = [t.orth_ for t in doc] # Crea una lista con las palabras del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = normalize(str_i) # crear una lista de tokens\n",
    "# stems = [spanishstemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_llamado_svm = svm.SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_llamado_svm.fit(x_transformed, respuesta_e) #x = x_transformer, y = respuesta_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 7 features, but SVC is expecting 24 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prediction \u001b[39m=\u001b[39m algo_llamado_svm\u001b[39m.\u001b[39;49mpredict(x_transformed2) \u001b[39m#acurracy test\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:820\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    818\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    819\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    821\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp))\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:433\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \n\u001b[0;32m    420\u001b[0m \u001b[39m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39m        The predicted values.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_for_predict(X)\n\u001b[0;32m    434\u001b[0m     predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_predict \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_predict\n\u001b[0;32m    435\u001b[0m     \u001b[39mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:613\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    610\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    612\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel):\n\u001b[1;32m--> 613\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    614\u001b[0m         X,\n\u001b[0;32m    615\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    616\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[0;32m    617\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    618\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    619\u001b[0m         reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sp\u001b[39m.\u001b[39misspmatrix(X):\n\u001b[0;32m    623\u001b[0m     X \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:569\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 569\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    571\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\violeta.macias\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:370\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 370\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 7 features, but SVC is expecting 24 features as input."
     ]
    }
   ],
   "source": [
    "prediction = algo_llamado_svm.predict(x_transformed2) #acurracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0%\n"
     ]
    }
   ],
   "source": [
    "print (u'Accuracy:  {:.1%}'.format(np.mean(prediction == respuesta_e)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secuential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = keras.models.Sequential() # Pag 532 cap 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirar keras.backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/qu4nt/reducir-el-n%C3%BAmero-de-palabras-de-un-texto-lematizaci%C3%B3n-y-radicalizaci%C3%B3n-stemming-con-python-965bfd0c69fa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://josearcosaneas.github.io/python/r/procesamiento/lenguaje/2017/01/02/procesamiento-lenguaje-natural-0.html#:~:text=El%20stemming%20consiste%20en%20extreaer,ayuda%20de%20la%20librer%C3%ADa%20NLTK."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentación Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/text/tutorials/nmt_with_attention?hl=es-419"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de chatbot:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://planetachatbot.com/tutorial-chatbot-usando-nltk-keras/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/api/utils/backend_utils/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://faroit.com/keras-docs/1.2.0/backend/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyectos de otra universidad:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://dspace.ups.edu.ec/bitstream/123456789/22069/1/UPS-CT009621.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://digital.csic.es/bitstream/10261/227679/1/TFM_VeronicaPinillaGomez%20%281%29.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://m.riunet.upv.es/bitstream/handle/10251/128834/Gallardo%20-%20Creaci%C3%B3n%20de%20un%20modelo%20predictivo%20para%20clasificar%20las%20consultas%20ciudadanas%20sobre%20el%20tra....pdf?sequence=1&isAllowed=y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ce52cbbbc6e028708724a15cf5ae56ba876571c2d68b4c149d42b015a691c4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
