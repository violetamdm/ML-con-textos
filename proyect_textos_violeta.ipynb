{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frases empalagosas San Valentin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports y downloads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports (modulos usados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\violeta.macias\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"popular\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación, preprocesamiento y análisis de datos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de los datos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_e = [[\"¡Te deseo un día de San Valentín increíble, mi amor! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad\"], \n",
    "[\"¡Feliz día de San Valentín, mi vida! quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón. Te amo con locura\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡Te deseo un día de San Valentín increíble, mi amor! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad', '¡Feliz día de San Valentín, mi vida! quiero que sepas cuanto te valoro y el gran lugar qué ocupas en mi corazón. Te amo con locura']\n"
     ]
    }
   ],
   "source": [
    "t_e = []\n",
    "for i in range(0, len(texto_e)):\n",
    "    t_e = t_e + texto_e[i]\n",
    "\n",
    "print(t_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_i = [[\"I love you.\"], [\"A hundred hearts would be too few to carry all my love for you.\"], [\"If you were thinking about someone while reading this, you're definitely in love.\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love you.', 'A hundred hearts would be too few to carry all my love for you.', \"If you were thinking about someone while reading this, you're definitely in love.\"]\n"
     ]
    }
   ],
   "source": [
    "t_i = []\n",
    "for i in range(0, len(texto_i)):\n",
    "    t_i = t_i + texto_i[i]\n",
    "\n",
    "print(t_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love you.A hundred hearts would be too few to carry all my love for you.If you were thinking about someone while reading this, you're definitely in love.\n"
     ]
    }
   ],
   "source": [
    "str_i = \"\"\n",
    "for i in range(0, len(t_i)):\n",
    "    str_i = str_i + str(t_i[i])\n",
    "\n",
    "print(str_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diccionario español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stop = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "print(spanish_stop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diccionario ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(english_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy \n",
    "from spacy.lang.en import English\n",
    "# nlp = spacy.load(es_core_news_sm)\n",
    "# doc = English(str_i) # Crea un objeto de spacy tipo nlp\n",
    "# tokens = [t.orth_ for t in doc] # Crea una lista con las palabras del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = normalize(str_i) # crear una lista de tokens\n",
    "# stems = [spanishstemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniza un texto a través de la creación de un vocabulario con las palabras que éste contiene.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texto = [[\"¡Te deseo un día de San Valentín increíble, mi amor! sigamos avanzando y trabajando por hacer de nuestros sueños una realidad\"], [\"\"]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1ª Forma de hacerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_v = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = count_v.fit_transform(t_e).toarray()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tx1 = count_v.fit_transform(texto[0]).toarray()\n",
    "tx2 = count_v.fit_transform(texto[1]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0 2 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0]\n",
      " [1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 2 0 1 0 1 1 1 0 1 1 0 0 2 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tx)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2ª Forma con stopwords una coleccion de spanish words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=spanish_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amo', 'amor', 'avanzando', 'corazón', 'cuanto', 'deseo', 'día',\n",
       "       'feliz', 'gran', 'hacer', 'increíble', 'locura', 'lugar', 'ocupas',\n",
       "       'quiero', 'realidad', 'san', 'sepas', 'sigamos', 'sueños',\n",
       "       'trabajando', 'valentín', 'valoro', 'vida'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_spanish = count_vectorizer.fit_transform(t_e).toarray()\n",
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3ª Forma de hacerlo:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.educative.io/answers/countvectorizer-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amo' 'amor' 'avanzando' 'con' 'corazón' 'cuanto' 'de' 'deseo' 'día' 'el'\n",
      " 'en' 'feliz' 'gran' 'hacer' 'increíble' 'locura' 'lugar' 'mi' 'nuestros'\n",
      " 'ocupas' 'por' 'que' 'quiero' 'qué' 'realidad' 'san' 'sepas' 'sigamos'\n",
      " 'sueños' 'te' 'trabajando' 'un' 'una' 'valentín' 'valoro' 'vida']\n"
     ]
    }
   ],
   "source": [
    "print(count_v.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx1 = count_v.fit(t_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'te': 29, 'deseo': 7, 'un': 31, 'día': 8, 'de': 6, 'san': 25, 'valentín': 33, 'increíble': 14, 'mi': 17, 'amor': 1, 'sigamos': 27, 'avanzando': 2, 'trabajando': 30, 'por': 20, 'hacer': 13, 'nuestros': 18, 'sueños': 28, 'una': 32, 'realidad': 24, 'feliz': 11, 'vida': 35, 'quiero': 22, 'que': 21, 'sepas': 26, 'cuanto': 5, 'valoro': 34, 'el': 9, 'gran': 12, 'lugar': 16, 'qué': 23, 'ocupas': 19, 'en': 10, 'corazón': 4, 'amo': 0, 'con': 3, 'locura': 15}\n"
     ]
    }
   ],
   "source": [
    "print(tx1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 36)\n",
      "[[0 1 1 0 0 0 2 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0]\n",
      " [1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 2 0 1 0 1 1 1 0 1 1 0 0 2 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vector = tx1.transform(t_e)\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21)\t0.2193806093508014\n",
      "  (0, 20)\t0.30833179183805903\n",
      "  (0, 19)\t0.30833179183805903\n",
      "  (0, 18)\t0.30833179183805903\n",
      "  (0, 16)\t0.2193806093508014\n",
      "  (0, 15)\t0.30833179183805903\n",
      "  (0, 10)\t0.30833179183805903\n",
      "  (0, 9)\t0.30833179183805903\n",
      "  (0, 6)\t0.2193806093508014\n",
      "  (0, 5)\t0.30833179183805903\n",
      "  (0, 2)\t0.30833179183805903\n",
      "  (0, 1)\t0.30833179183805903\n",
      "  (1, 23)\t0.2719769017589197\n",
      "  (1, 22)\t0.2719769017589197\n",
      "  (1, 21)\t0.19351380563621104\n",
      "  (1, 17)\t0.2719769017589197\n",
      "  (1, 16)\t0.19351380563621104\n",
      "  (1, 14)\t0.2719769017589197\n",
      "  (1, 13)\t0.2719769017589197\n",
      "  (1, 12)\t0.2719769017589197\n",
      "  (1, 11)\t0.2719769017589197\n",
      "  (1, 8)\t0.2719769017589197\n",
      "  (1, 7)\t0.2719769017589197\n",
      "  (1, 6)\t0.19351380563621104\n",
      "  (1, 4)\t0.2719769017589197\n",
      "  (1, 3)\t0.2719769017589197\n",
      "  (1, 0)\t0.2719769017589197\n"
     ]
    }
   ],
   "source": [
    "x_transformed = tfidf_transformer.fit_transform(text_spanish)\n",
    "print(x_transformed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://es.linkedin.com/learning/python-para-data-scientist-avanzado/stemming-procesamiento-textual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ya sepas cómo eliminar todo el ruido más superficial de un texto como, por ejemplo, las \"stop words\", aquellas palabras que sencillamente no aportan información porque son conjunciones o preposiciones, y todos los signos de puntuación, el siguiente paso normalmente en un análisis de texto es utilizar un \"stemmer\". ¿Qué es exactamente un \"stemmer\"? Aquí te propongo usar este, que forma parte del NLTK. Y lo que estamos haciendo cuando usamos una función de este estilo es reducir las palabras a la raíz. Vamos a ver qué es lo que estamos haciendo. Aquí tenemos las palabras sin procesar, y una de las cosas que podemos hacer es reducir todas las conjugaciones del verbo \"llegar\" a su raíz. Es decir, llevar todas las palabras que sean \"llegaba\", \"llegaban\", etc., a \"llegar\". Esto es lo que hace una función de este estilo. Tenemos que inicializar 'stemmer' en el idioma que lo queremos. Hay bastantes más. El tema es que muchos de ellos no aceptan el español, así que yo recomiendo este si estás trabajando con texto en castellano, pero sabed que existe, por ejemplo, también el PorterStemmer, que también es muy potente. Aquí creamos el objeto. Y lo que vamos a hacer es, usando este \"loop\", coger cada una de las palabras que tengamos en la lista y vamos a extraer su raíz. He creado una lista de raíces, de momento vacía. Y vamos a extraer la raíz de cada una de ellas. Como puedes ver, es inmediato. Tenemos exactamente el mismo texto, pero hemos despojado a todas las palabras de sus sufijos, es decir, tenemos la raíz. Antes he hablado de \"llegar\" y ahora solo tenemos la raíz. Esto lo que permite es que, cuando hagamos un recuento de frecuencias, no nos veamos afectados por qué conjugación o qué variante de la palabra estemos usando. Si lo que queremos es usar la función 'FreqDist', de las raíces, cogemos, por ejemplo, las 20 primeras. Vemos efectivamente que no son las palabras en realidad, aunque hay algunas que sí. Señor, por ejemplo, tiene el mismo formato. Pero si este objeto encuentra \"señora\", también lo va a reducir a \"señor\". Este es el punto. Y comparamos esto con... Y podemos ver, por ejemplo, que \"caballero\" ahora es la palabra que más aparece, o la raíz asociada a \"caballero\", y aquí no lo era, estaba empatada con \"nombre\". Este proceso que, como has visto, es increíblemente sencillo, consta de crear un objeto y recorrer mediante un bucle todas las palabras que tenemos en nuestra lista. Recomiendo que primero lo limpies con una instrucción de este tipo. Y creamos una transformación de nuestras palabras que permite realizar un análisis mucho más específico que no dependa tanto de las circunstancias concretas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce palabras a su raíz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_in = str(word_tokenize(str_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[:[\n",
      "':'\n",
      "I:i\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "l:l\n",
      "v:v\n",
      "e:e\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "u:u\n",
      ".:.\n",
      "A:a\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "h:h\n",
      "u:u\n",
      "n:n\n",
      "r:r\n",
      "e:e\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "h:h\n",
      "e:e\n",
      "r:r\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "w:w\n",
      "u:u\n",
      "l:l\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "b:b\n",
      "e:e\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "f:f\n",
      "e:e\n",
      "w:w\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "c:c\n",
      "r:r\n",
      "r:r\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "l:l\n",
      "l:l\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "l:l\n",
      "v:v\n",
      "e:e\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "f:f\n",
      "r:r\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "u:u\n",
      ".:.\n",
      "I:i\n",
      "f:f\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "u:u\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "w:w\n",
      "e:e\n",
      "r:r\n",
      "e:e\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "h:h\n",
      "n:n\n",
      "k:k\n",
      "n:n\n",
      "g:g\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "b:b\n",
      "u:u\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "e:e\n",
      "n:n\n",
      "e:e\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "w:w\n",
      "h:h\n",
      "l:l\n",
      "e:e\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "r:r\n",
      "e:e\n",
      "n:n\n",
      "g:g\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "h:h\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      ",:,\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "u:u\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "\":\"\n",
      "':'\n",
      "r:r\n",
      "e:e\n",
      "\":\"\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "e:e\n",
      "f:f\n",
      "n:n\n",
      "e:e\n",
      "l:l\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "n:n\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      "l:l\n",
      "v:v\n",
      "e:e\n",
      "':'\n",
      ",:,\n",
      " : \n",
      "':'\n",
      ".:.\n",
      "':'\n",
      "]:]\n"
     ]
    }
   ],
   "source": [
    "tx_in = [p for p in tx_in if not p in set(english_stopwords)]\n",
    "for p in tx_in:\n",
    "    print(p + \":\" + ps.stem(p))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematización"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lematización agrega contexto a las palabras, es preferible al stemming.\n",
    "\n",
    "Es una etapa importante en el PLN, está técnica algorítmica consiste en\n",
    "encontrar la forma morfológica básica de una palabra (lema), para que\n",
    "esto suceda se agrupan las palabras flexionadas que vendrían siendo\n",
    "verbos conjugados como: limpiar – limpiando, palabras en distintas\n",
    "formas como: gato – gata, entre otros, y se analiza como un solo elemento\n",
    "para encontrar la palabra base (Dereza, 2018) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I                   I                   \n",
      "                                        \n",
      "l                   l                   \n",
      "o                   o                   \n",
      "v                   v                   \n",
      "e                   e                   \n",
      "                                        \n",
      "y                   y                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      ".                   .                   \n",
      "A                   A                   \n",
      "                                        \n",
      "h                   h                   \n",
      "u                   u                   \n",
      "n                   n                   \n",
      "d                   d                   \n",
      "r                   r                   \n",
      "e                   e                   \n",
      "d                   d                   \n",
      "                                        \n",
      "h                   h                   \n",
      "e                   e                   \n",
      "a                   a                   \n",
      "r                   r                   \n",
      "t                   t                   \n",
      "s                   s                   \n",
      "                                        \n",
      "w                   w                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      "l                   l                   \n",
      "d                   d                   \n",
      "                                        \n",
      "b                   b                   \n",
      "e                   e                   \n",
      "                                        \n",
      "t                   t                   \n",
      "o                   o                   \n",
      "o                   o                   \n",
      "                                        \n",
      "f                   f                   \n",
      "e                   e                   \n",
      "w                   w                   \n",
      "                                        \n",
      "t                   t                   \n",
      "o                   o                   \n",
      "                                        \n",
      "c                   c                   \n",
      "a                   a                   \n",
      "r                   r                   \n",
      "r                   r                   \n",
      "y                   y                   \n",
      "                                        \n",
      "a                   a                   \n",
      "l                   l                   \n",
      "l                   l                   \n",
      "                                        \n",
      "m                   m                   \n",
      "y                   y                   \n",
      "                                        \n",
      "l                   l                   \n",
      "o                   o                   \n",
      "v                   v                   \n",
      "e                   e                   \n",
      "                                        \n",
      "f                   f                   \n",
      "o                   o                   \n",
      "r                   r                   \n",
      "                                        \n",
      "y                   y                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      ".                   .                   \n",
      "I                   I                   \n",
      "f                   f                   \n",
      "                                        \n",
      "y                   y                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      "                                        \n",
      "w                   w                   \n",
      "e                   e                   \n",
      "r                   r                   \n",
      "e                   e                   \n",
      "                                        \n",
      "t                   t                   \n",
      "h                   h                   \n",
      "i                   i                   \n",
      "n                   n                   \n",
      "k                   k                   \n",
      "i                   i                   \n",
      "n                   n                   \n",
      "g                   g                   \n",
      "                                        \n",
      "a                   a                   \n",
      "b                   b                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      "t                   t                   \n",
      "                                        \n",
      "s                   s                   \n",
      "o                   o                   \n",
      "m                   m                   \n",
      "e                   e                   \n",
      "o                   o                   \n",
      "n                   n                   \n",
      "e                   e                   \n",
      "                                        \n",
      "w                   w                   \n",
      "h                   h                   \n",
      "i                   i                   \n",
      "l                   l                   \n",
      "e                   e                   \n",
      "                                        \n",
      "r                   r                   \n",
      "e                   e                   \n",
      "a                   a                   \n",
      "d                   d                   \n",
      "i                   i                   \n",
      "n                   n                   \n",
      "g                   g                   \n",
      "                                        \n",
      "t                   t                   \n",
      "h                   h                   \n",
      "i                   i                   \n",
      "s                   s                   \n",
      ",                   ,                   \n",
      "                                        \n",
      "y                   y                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      "'                   '                   \n",
      "r                   r                   \n",
      "e                   e                   \n",
      "                                        \n",
      "d                   d                   \n",
      "e                   e                   \n",
      "f                   f                   \n",
      "i                   i                   \n",
      "n                   n                   \n",
      "i                   i                   \n",
      "t                   t                   \n",
      "e                   e                   \n",
      "l                   l                   \n",
      "y                   y                   \n",
      "                                        \n",
      "i                   i                   \n",
      "n                   n                   \n",
      "                                        \n",
      "l                   l                   \n",
      "o                   o                   \n",
      "v                   v                   \n",
      "e                   e                   \n",
      ".                   .                   \n"
     ]
    }
   ],
   "source": [
    "for p in str_i:\n",
    "    print(\"{0:20}{1:20}\".format(p, wordnet_lemmatizer.lemmatize(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I                   I                   \n",
      "                                        \n",
      "l                   l                   \n",
      "o                   o                   \n",
      "v                   v                   \n",
      "e                   e                   \n",
      "                                        \n",
      "y                   y                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      ".                   .                   \n",
      "A                   A                   \n",
      "                                        \n",
      "h                   h                   \n",
      "u                   u                   \n",
      "n                   n                   \n",
      "d                   d                   \n",
      "r                   r                   \n",
      "e                   e                   \n",
      "d                   d                   \n",
      "                                        \n",
      "h                   h                   \n",
      "e                   e                   \n",
      "a                   a                   \n",
      "r                   r                   \n",
      "t                   t                   \n",
      "s                   s                   \n",
      "                                        \n",
      "w                   w                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      "l                   l                   \n",
      "d                   d                   \n",
      "                                        \n",
      "b                   b                   \n",
      "e                   e                   \n",
      "                                        \n",
      "t                   t                   \n",
      "o                   o                   \n",
      "o                   o                   \n",
      "                                        \n",
      "f                   f                   \n",
      "e                   e                   \n",
      "w                   w                   \n",
      "                                        \n",
      "t                   t                   \n",
      "o                   o                   \n",
      "                                        \n",
      "c                   c                   \n",
      "a                   a                   \n",
      "r                   r                   \n",
      "r                   r                   \n",
      "y                   y                   \n",
      "                                        \n",
      "a                   a                   \n",
      "l                   l                   \n",
      "l                   l                   \n",
      "                                        \n",
      "m                   m                   \n",
      "y                   y                   \n",
      "                                        \n",
      "l                   l                   \n",
      "o                   o                   \n",
      "v                   v                   \n",
      "e                   e                   \n",
      "                                        \n",
      "f                   f                   \n",
      "o                   o                   \n",
      "r                   r                   \n",
      "                                        \n",
      "y                   y                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      ".                   .                   \n",
      "I                   I                   \n",
      "f                   f                   \n",
      "                                        \n",
      "y                   y                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      "                                        \n",
      "w                   w                   \n",
      "e                   e                   \n",
      "r                   r                   \n",
      "e                   e                   \n",
      "                                        \n",
      "t                   t                   \n",
      "h                   h                   \n",
      "i                   i                   \n",
      "n                   n                   \n",
      "k                   k                   \n",
      "i                   i                   \n",
      "n                   n                   \n",
      "g                   g                   \n",
      "                                        \n",
      "a                   a                   \n",
      "b                   b                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      "t                   t                   \n",
      "                                        \n",
      "s                   s                   \n",
      "o                   o                   \n",
      "m                   m                   \n",
      "e                   e                   \n",
      "o                   o                   \n",
      "n                   n                   \n",
      "e                   e                   \n",
      "                                        \n",
      "w                   w                   \n",
      "h                   h                   \n",
      "i                   i                   \n",
      "l                   l                   \n",
      "e                   e                   \n",
      "                                        \n",
      "r                   r                   \n",
      "e                   e                   \n",
      "a                   a                   \n",
      "d                   d                   \n",
      "i                   i                   \n",
      "n                   n                   \n",
      "g                   g                   \n",
      "                                        \n",
      "t                   t                   \n",
      "h                   h                   \n",
      "i                   i                   \n",
      "s                   s                   \n",
      ",                   ,                   \n",
      "                                        \n",
      "y                   y                   \n",
      "o                   o                   \n",
      "u                   u                   \n",
      "'                   '                   \n",
      "r                   r                   \n",
      "e                   e                   \n",
      "                                        \n",
      "d                   d                   \n",
      "e                   e                   \n",
      "f                   f                   \n",
      "i                   i                   \n",
      "n                   n                   \n",
      "i                   i                   \n",
      "t                   t                   \n",
      "e                   e                   \n",
      "l                   l                   \n",
      "y                   y                   \n",
      "                                        \n",
      "i                   i                   \n",
      "n                   n                   \n",
      "                                        \n",
      "l                   l                   \n",
      "o                   o                   \n",
      "v                   v                   \n",
      "e                   e                   \n",
      ".                   .                   \n"
     ]
    }
   ],
   "source": [
    "for p in str_i:\n",
    "    print(\"{0:20}{1:20}\".format(p, wordnet_lemmatizer.lemmatize(p, pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/qu4nt/reducir-el-n%C3%BAmero-de-palabras-de-un-texto-lematizaci%C3%B3n-y-radicalizaci%C3%B3n-stemming-con-python-965bfd0c69fa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentación Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de chatbot:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://planetachatbot.com/tutorial-chatbot-usando-nltk-keras/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyectos de otra universidad:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://dspace.ups.edu.ec/bitstream/123456789/22069/1/UPS-CT009621.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://digital.csic.es/bitstream/10261/227679/1/TFM_VeronicaPinillaGomez%20%281%29.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://m.riunet.upv.es/bitstream/handle/10251/128834/Gallardo%20-%20Creaci%C3%B3n%20de%20un%20modelo%20predictivo%20para%20clasificar%20las%20consultas%20ciudadanas%20sobre%20el%20tra....pdf?sequence=1&isAllowed=y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ce52cbbbc6e028708724a15cf5ae56ba876571c2d68b4c149d42b015a691c4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
